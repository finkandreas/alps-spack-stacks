{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CSCS User Environments","text":"<p>Documentation for the user-environments provided on CSCS Alps infrastructure.</p>"},{"location":"#writing-documentation","title":"Writing Documentation","text":"<p>The documentation for uenv is written with Material for mkdocs.</p> <p>See the documentation for Material for details on all of the features that it provides.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To view the documentation as you write, you can install Material for mkdocs using pip.</p> <p>First, create Python virtual environment and install <code>mkdocs-material</code> <pre><code># create and activate a venv\npython3 -m venv pyenv\nsource pyenv/bin/activate\n\n# install required packages\npip install --upgrade pip\npip install mkdocs-material\n</code></pre></p> <p>Note</p> <p>If you just created the python virtual environment, you might have to restart it for the <code>mkdocs</code> executable to be added to <code>PATH</code>.</p> <pre><code># you have to deactivate and start again for mkdocs to be available\ndeactivate\nsource pyenv/bin/activate\n</code></pre> <p>The documentation is built using the <code>mkdocs</code> executable. To view your documentation in your browser, run the mkdocs server: <pre><code>mkdocs serve\n</code></pre></p> <p>Leave it running, and every time you save the markdown file the docs will be regenerated.</p> <p>The docs can be viewed by using the link printed by <code>mkdocs serve</code>:</p> <pre><code>mkdocs serve                                                                                                  (pyenv) main [e2006ad] \u0394 ?\nINFO    -  Building documentation...\nINFO    -  Cleaning site directory\nINFO    -  The following pages exist in the docs directory, but are not included in the \"nav\" configuration:\n             - writing-docs.md\nINFO    -  Documentation built in 0.15 seconds\nINFO    -  [12:17:14] Watching paths for changes: 'docs', 'mkdocs.yml'\nINFO    -  [12:17:14] Serving on http://127.0.0.1:8000/\n</code></pre> <p>And viewing it ( http://127.0.0.1:8000/ typically) in your local browser.</p>"},{"location":"#documentation-layout","title":"Documentation Layout","text":"<p>To add documentation for a uenv, create a file <code>docs/uenv-&lt;uenv name&gt;.md</code>, and look at existing documentation to get started.</p> <p>For your docs to be generated, it has to be added to the index in the <code>mkdocs.yml</code> file in the root of the repository. For example, to add docs for a new environment called <code>wombat</code>:</p> <pre><code>nav:\n  - Home: index.md\n  - 'uenv':\n    - 'Overview': uenv-overview.md\n    - 'wombat': uenv-wombat.md\n</code></pre>"},{"location":"pkg-application-tutorial/","title":"Application Packaging Tutorial","text":"<p>Target audience</p> <p>The target audience for this tutorial is CSCS staff who want to provide an application or programming environment uenv for users on Alps. We assume that you are familiar with Spack and how to build uenv using Stackinator - the focus here will be on best practices.</p> <p>This tutorial walks through configuring and maintaining a uenv recipe and deployment a representative HPC application. The uenv has to support some common use cases, and should cover most of the aspects of deploying your own uenv.</p> <p>Arbor is a scientific software for neuroscience simulation, with features including:</p> <ul> <li>A C++ library with a Python interface</li> <li>Distributed execution through MPI</li> <li>Multicore simulation</li> <li>Support for both NVIDIA and AMD GPUs</li> </ul>"},{"location":"pkg-application-tutorial/#requirements","title":"Requirements","text":"<p>Before starting, we gather requirements for the use cases of the uenv on the system, in order to understand:</p> <ul> <li>which packages the uenv will provide;</li> <li>which interfaces the uenv will provide to those packages.</li> </ul>"},{"location":"pkg-application-tutorial/#supported-workflows","title":"Supported workflows","text":"<p>For Arbor we wish to support two workflows:</p> <ul> <li>Application: provide optimised builds of Arbor for users to use directly</li> <li>BYO/Developer: Arbor is under active development, and some users require the ability to build the latest bleeding edge version, or build with a customised configuration. This workflow also supports Arbor developers, who need to implement new features, fix bugs and test on Alps.</li> </ul>"},{"location":"pkg-application-tutorial/#supported-systems","title":"Supported systems","text":"<p>Arbor is well-optimised for both CPU and GPU execution and users of systems with and without accelerators, so we will provide it for the following platforms:</p> <ul> <li>multicore: <code>zen2</code>/<code>zen3</code></li> <li><code>gh200</code></li> </ul> <p>supported platforms</p> <p>Supported targets on Alps are currently <code>zen2</code>, <code>zen3</code>, <code>a100</code>, <code>mi200</code>, and <code>gh200</code>.</p> <p>For more information, see the internal CSCS confluence. Also, for information about which targets are available on which vClusters, see the <code>config.yaml</code>.</p>"},{"location":"pkg-application-tutorial/#compilers","title":"Compilers","text":"<p>Arbor is a C++17 library that officially supports GCC and Clang, with a Python front end.</p> <p>For this we choose the following compiler versions:</p> target compiler cuda python <code>zen2</code>/<code>zen3</code> <code>gcc@13.2</code> - <code>python@3.11</code> <code>gh200</code> <code>gcc@13.2</code> <code>cuda@12.4</code> <code>python@3.11</code>"},{"location":"pkg-application-tutorial/#packages","title":"Packages","text":"<p>The first step when building an application, use-case or workflow uenv is to determine which specs to add to the list.</p> <p>If the aim was to provide arbor with cuda and Python support enabled, an <code>environments.yaml</code> file that provides a single spec <code>arbor@0.9 +python</code> could be sufficient, e.g.:</p> simple environments.yaml<pre><code>arbor:\n  compiler:\n      - toolchain: gcc\n        spec: gcc\n  mpi:\n      spec: cray-mpich\n  unify: true\n  specs:\n  - arbor@0.9 +python\n  variants:\n  - +mpi\n  - +cuda\n  - cuda_arch=90\n  views:\n    arbor:\n      links: root\n</code></pre> <p>This environment definition will build arbor, with all of its dependencies implicitly concretised by Spack. Such a simple recipe is sometimes sufficient, however one will often need to provide a more detailed set of specs. Reasons for more detailed specs include:</p> <ul> <li>to pin the version of a specific dependency, e.g.:<ul> <li>to ensure that the version of a package is not dependent on which version of Spack is used;</li> <li>to a version that is well supported and tested on the target system;</li> <li>to a version that patches a bug on the target system.</li> </ul> </li> <li>to specialise the spec of a specific dependency, e.g.:<ul> <li>with non-default variants that support all features on the target system;</li> <li>with non-default variants that give the best performance on the target system;</li> <li>to use a specific compiler when more than one compiler toolchain is used to build packages in an environment.</li> </ul> </li> <li>to explicitly list all of the dependencies to provide to users in an environment view</li> </ul> <p>The objective for the Arbor uenv is to provide both Arbor and all of the tools and libraries to \"build your own\" Arbor. This requires providing all of the libraries and tools required to download the Arbor source code, run CMake, and build in a file system view.</p> <p>As a starting point, we use the spack package for Arbor. From this we derive a list of dependencies:</p> <ul> <li>direct dependencies like <code>pugixml</code>, <code>fmt</code> and <code>pybind11</code> needed to build Arbor.</li> <li>compiler and languages like <code>python</code> and <code>cuda</code>.</li> </ul>"},{"location":"pkg-application-tutorial/#the-recipe","title":"The Recipe","text":"<p>With requirements in hand, it is now time to write the recipe.</p>"},{"location":"pkg-application-tutorial/#config","title":"Config","text":"<p>There are a few simple choices to make when writing the <code>config.yaml</code> file:</p> <code>name</code> <p>Keep it simple, we choose <code>arbor</code>.</p> <p>Tip</p> <p>Use the same name on all hardware targets, i.e. use a name like <code>arbor</code> or <code>gromacs</code> instead of <code>arbor-gpu</code> or <code>gromacs-x86</code>. By doing this users can more easily find your uenv on all vClusters - if they are on a system with an x86 CPU, they can assume that the <code>arbor</code> uenv has been built appropriately.</p> <p>The uenv CLI tool also allows users to disambiguate which micro-architecture they require, if on a system that provides versions of a uenv built for multiple uarch:</p> <pre><code>uenv image ls --uarch=gh200 arbor\nuenv image ls --uarch=zen2 arbor\n</code></pre> <code>spack</code> <p>By default use the most recent version of Spack supported by Stackinator. At the time of writing, the most recent version of Spack is <code>v0.21</code>, for which it is recommended to use the <code>releases/v0.21</code> branch, which receives backports of bug fixes while not changing the API or recipe definitions.</p> <p>Warning</p> <p>The <code>develop</code> branch should be avoided for uenv deployed on CSCS clusters unless it is absolutely necessary.</p> <code>mount</code> <p>Normally application and development uenvs go in <code>/user-environment</code> and tools that you might want to use alongside a development or application uenv go in <code>/user-tools</code> (e.g. a debugger). For Arbor, we choose the default <code>/user-environment</code> path.</p> <code>description</code> <p>Keep it simple, fit it on one line.</p> <code>mc</code><code>gh200</code> config.yaml<pre><code>name: arbor\nspack:\n  commit: releases/v0.21\n  repo: https://github.com/spack/spack.git\nstore: /user-environment\ndescription: The Arbor neuroscience simulation package and its dependencies for multicore systems.\n</code></pre> config.yaml<pre><code>name: arbor\nspack:\n  commit: releases/v0.21\n  repo: https://github.com/spack/spack.git\nstore: /user-environment\ndescription: The Arbor neuroscience simuluation package and its dependencies for Grace-Hopper.\n</code></pre>"},{"location":"pkg-application-tutorial/#compilers_1","title":"Compilers","text":"<p>Based on our requirements above, defining compilers is straightforward.</p> <code>mc</code><code>gh200</code> compilers.yaml<pre><code>bootstrap:\n  spec: gcc@12.3\ngcc:\n  specs:\n  - gcc@13.2\n</code></pre> compilers.yaml<pre><code>bootstrap:\n  spec: gcc@12.3\ngcc:\n  specs:\n  - gcc@13.2\n</code></pre>"},{"location":"pkg-application-tutorial/#environments","title":"Environments","text":"<p>The environment definitions include the specs that we want to provide to end users, and the relevant <code>cuda</code> and <code>python</code> versions depending on each application.</p> <code>mc</code><code>gh200</code> environments.yaml<pre><code>arbor:\n  compiler:\n      - toolchain: gcc\n        spec: gcc\n  mpi:\n      spec: cray-mpich@8.1.29\n  unify: true\n  specs:\n  # arbor\n  - arbor@0.9 +python\n  # build tools\n  - cmake\n  - googletest\n  - ninja\n  - python@3.11\n  # C++ dependencies\n  - fmt\n  - pugixml\n  - nlohmann-json\n  - random123\n  # python packages\n  - py-mpi4py\n  - py-numpy\n  - py-pip\n  - py-pybind11\n  # etc\n  - osu-micro-benchmarks\n  variants:\n  - +mpi\n  packages:\n  - diffutils\n  - gettext\n  - gmake\n  - libxml2\n  - perl\n  views:\n    arbor:\n      link: roots\n    develop:\n      link: roots\n      exclude: [arbor]\n</code></pre> environments.yaml<pre><code>arbor:\n  compiler:\n      - toolchain: gcc\n        spec: gcc\n  mpi:\n      spec: cray-mpich@8.1.29\n      gpu: cuda\n  unify: true\n  specs:\n  # arbor\n  - arbor@0.9 +python\n  # build tools\n  - cuda@12.4\n  - cmake\n  - googletest\n  - ninja\n  - python@3.11\n  # C++ dependencies\n  - fmt\n  - pugixml\n  - nlohmann-json\n  - random123\n  # python packages\n  - py-mpi4py\n  - py-numpy\n  - py-pip\n  - py-pybind11\n  # etc\n  - osu-micro-benchmarks\n  variants:\n  - +mpi\n  - +cuda\n  - cuda_arch=90\n  packages:\n  - diffutils\n  - gettext\n  - gmake\n  - libxml2\n  - perl\n  views:\n    arbor:\n      link: roots\n    develop:\n      link: roots\n      exclude: [arbor]\n</code></pre> <p>variants</p> <p>Environments on GH200 will typically have the following variants set:</p> <ul> <li><code>+cuda</code> sets that variant for all that support it, required for NVIDIA GPU builds.</li> <li><code>cuda_arch=90</code> is required for <code>gh200</code> (use <code>cuda_arch=80</code> for the <code>a100</code> nodes)</li> </ul> <p>views and roots</p> <p>Always use <code>view:link:roots</code> if possible to filter which packages are added to views. The default <code>all</code> setting and also the <code>run</code> setting can add a lot of packages that were not explicitly in the list of your uenv's specs.</p> <p>Packages in the view can lead to conflicts, which can be avoided by only including packages that are strictly required. For example, if a view has a common dependency like <code>libssl</code> in its <code>/lib</code> path, and <code>LD_LIBRARY_PATH</code> is set, system CLI tools like <code>git</code> can crash because the link against the <code>libssl</code> in the uenv at runtime.</p>"},{"location":"pkg-application-tutorial/#modules","title":"Modules","text":"<p>We add a module file, which controls which modules are provided by the uenv. This is because some users might want modules, and it doesn't hurt to provide them (this is a weak reason, and we accept that we will be on the hook for supporting them for users who incorporate them into their workflows).</p> <p>Info</p> <p>If you don't need to provide modules, set <code>modules: False</code> in <code>config.yaml</code>.</p> mcgh200 modules.yaml<pre><code>modules:\n  # Paths to check when creating modules for all module sets\n  prefix_inspections:\n    bin:\n      - PATH\n    lib:\n      - LD_LIBRARY_PATH\n    lib64:\n      - LD_LIBRARY_PATH\n\n  default:\n    arch_folder: false\n    # Where to install modules\n    roots:\n      tcl: /user-environment/modules\n    tcl:\n      all:\n        autoload: none\n      hash_length: 0\n      exclude_implicits: true\n      exclude: ['%gcc@7.5.0', 'gcc %gcc@7.5.0']\n      projections:\n        all: '{name}/{version}'\n</code></pre> modules.yaml<pre><code>modules:\n  # Paths to check when creating modules for all module sets\n  prefix_inspections:\n    bin:\n      - PATH\n    lib:\n      - LD_LIBRARY_PATH\n    lib64:\n      - LD_LIBRARY_PATH\n\n  default:\n    arch_folder: false\n    # Where to install modules\n    roots:\n      tcl: /user-environment/modules\n    tcl:\n      all:\n        autoload: none\n      hash_length: 0\n      exclude_implicits: true\n      exclude: ['%gcc@7.5.0', 'gcc %gcc@7.5.0']\n      projections:\n        all: '{name}/{version}'\n</code></pre>"},{"location":"pkg-application-tutorial/#testing","title":"Testing","text":"<p>Failure</p> <p>write reframe tests.</p>"},{"location":"pkg-application-tutorial/#deployment","title":"Deployment","text":""},{"location":"pkg-application-tutorial/#configuring-the-pipeline","title":"Configuring the pipeline","text":"<p>The target systems for deploying the Arbor uenv to users are Eiger (<code>zen2</code>) and Santis (<code>gh200</code>).</p> <p>To enable the CI/CD pipeline to build and deploy the uenv on these systems, update the <code>config.yaml</code> file in the alps-uenv repository:</p> <pre><code>uenvs:\n  arbor:\n    v0.9:\n      recipes:\n        zen2: v0.9/mc\n        gh200: v0.9/gh200\n      deploy:\n        eiger: [zen2]\n        santis: [gh200]\n</code></pre> <p>Tip</p> <p>To test that the pipeline yaml is correctly configured before pushing the changes and making a PR, you can run a basic test with the new uenv:</p> <pre><code>system=santis uarch=gh200 uenv=arbor:v0.9 ./ci/configure-pipeline\nsystem=eiger uarch=zen2 uenv=arbor:v0.9 ./ci/configure-pipeline\n</code></pre> <p>If there are no obvious error messages, you are good to go!</p>"},{"location":"pkg-application-tutorial/#running-the-pipeline","title":"Running the pipeline","text":"<p>To run the pipeline that will automatically build and test your uenv, first create a PR:</p> <ol> <li>Push your changes to a branch (preferably in a fork of the main alps-uenv repository).</li> <li>Open a PR with your changes.</li> </ol> <p>Once the PR is created, the pipeline has to be triggered for each individual combination of uenv/version/uarch/vCluster by using a specially formatted</p> <pre><code>cscs-ci run alps;system=eiger;uarch=zen2;uenv=arbor:v0.9\ncscs-ci run alps;system=santis;uarch=gh200;uenv=arbor:v0.9\n</code></pre>"},{"location":"pkg-application-tutorial/#checking-the-build","title":"Checking the build","text":"<p>Log onto the target system, e.g. <code>santis</code>, and use the <code>uenv image find --build</code> command to search for the build.</p> <pre><code>&gt; uenv image find --build arbor\nuenv/version:tag                        uarch date       id               size\narbor/v0.9:1250847801                   gh200 2024-04-12 89c9a36f21b496a2 3.6GB\narbor/v0.9:1249535229                   gh200 2024-04-11 0a2d82448ecaafd7 3.6GB\n</code></pre> <p>Info</p> <p>The <code>--build</code> flag is required with the <code>find</code> and <code>pull</code> commands to interact with images that have been built by the pipeline, but not yet deployed.</p> <p>Pick the version to pull (if it isn't clear which version to pull, inspect the logs of the CI/CD job that built the image).</p> <pre><code># pull the image using its id\nuenv image pull --build 89c9a36f21b496a2\n\n# then start the image to test it\nuenv image start 89c9a36f21b496a2\n</code></pre>"},{"location":"pkg-application-tutorial/#docs","title":"Docs","text":"<p>Failure</p> <p>Write about how to document.</p>"},{"location":"uenv-compilation-spack/","title":"Using uenvs as upstream Spack instances","text":"<p>User-environments (uenvs) are built with Spack using the Stackinator tool. Therefore, a uenv is tightly coupled with Spack and can be used as an upstream Spack instance (see Chaining Spack Installations for more details).</p> <p>Note</p> <p>While this guide tries to explain everything step-by-step, it might be difficult to follow without any knowledge of Spack. Please have a look at Spack Basic Usage for a short introduction to Spack.</p> <p>The uenv of a supported application contains all the dependencies to build the software with a particular configuration. In Spack, such configuration is defined by a spec (see Spack Basic Usage for more details). Most uenvs already provide modules or Spack Filesystem Views which allow to manually build the same configuration. However, it is possible that you want to build or develop an application with a different configuration. To avoid re-building the whole uenv, you can re-use what is already there and build your new configuration using Spack. </p> <p>This guide explains a developer workflow allowing to either build your own package with Spack, or use Spack to build all the dependencies and provide a build environment for the package.</p> <p>Note</p> <p>This guide assumes that you have a local installation of Spack. If you don't have Spack installed, follow Spack Getting Started.</p> <p>Tip</p> <p>To avoid compatibility issues, try to match the version of your local Spack instance with the version of Spack of the uenv. You can use the following command to clone the same Spack version used by the uenv: <pre><code>git clone \\\n    -b $(jq -r .spack.commit /user-environment/meta/configure.json) \\\n    $(jq -r .spack.repo /user-environment/meta/configure.json) $SCRATCH/spack\n</code></pre></p> <p>Warning</p> <p>Avoid installing Spack on <code>HOME</code>. Packages are installed within the <code>spack/</code> folder, and you might quickly run out of space. Use <code>SCRATCH</code> instead.</p> <p>Danger</p> <p>The recommendation to use <code>SCRATCH</code> to install your local Spack instance(s) might change in the future. Make sure you are aware of our <code>SCRATCH</code> cleaning policy. </p>"},{"location":"uenv-compilation-spack/#example-quantum-espresso","title":"Example: Quantum ESPRESSO","text":"<p>As an example, we will consider Quantum ESPRESSO as the application you want to develop. Let's assume that the provided configuration in the official uenv is the following:</p> <pre><code>quantum-espresso@7.3.1 %nvhpc +libxc +cuda cuda_arch=90\n</code></pre> <p>This spec defines a build of Quantum ESPRESSO version <code>7.3.1</code> using the <code>nvhpc</code> compiler. All other dependencies and features are defined by the default values in the Quantum ESPRESSO Spack package.</p>"},{"location":"uenv-compilation-spack/#set-uenv-as-upstream-spack-instance","title":"Set uenv as upstream Spack instance","text":"<p>Here we assume the uenv described above is called <code>quantumespresso/v7.3.1</code> and it is already deployed. You can therefore pull the <code>quantumespresso/v7.3.1</code> image and start the uenv as follows:</p> <pre><code>uenv image pull quantumespresso/v7.3.1\nuenv start quantumespresso/v7.3.1\n</code></pre> <p>With the uenv active, you can now tell your local Spack instance to use the uenv as an upstream Spack instance (see Chaining Spack Installations for more details):</p> <pre><code>export SPACK_SYSTEM_CONFIG_PATH=/user-environment/config/\n</code></pre> <p>Note</p> <p>We assumed here that the uenv is mounted in the standard location <code>/user-environment/</code>. If it is mounted in a non-standard location, adjust the previous command accordingly.</p>"},{"location":"uenv-compilation-spack/#building-your-own-version","title":"Building your own version","text":"<p>Let's assume you want to have a version of Quantum ESPRESSO with GPU-aware MPI:</p> <pre><code>quantum-espresso@7.3.1 %nvhpc +libxc +cuda cuda_arch=90 +mpigpu\n</code></pre> <p>This variant of Quantum ESPRESSO is not available in the uenv.</p>"},{"location":"uenv-compilation-spack/#spack-environment","title":"Spack Environment","text":"<p>To make things clean and reproducible, you can use Spack Environments to describe what you want to build. To define a Spack environment you have to create the following file, named <code>spack.yaml</code>, in a folder (hereafter referred to as <code>SPACK_ENV_FOLDER</code>):</p> <pre><code>spack:\n  specs:\n  -  quantum-espresso@7.3.1 %nvhpc +libxc +mpigpu\n  packages:\n    all:\n      prefer:\n        - +cuda cuda_arch=90\n  view: false\n  concretizer:\n    unify: true\n</code></pre> <p><code>packages:all:prefer</code> indicates that you want the <code>+cuda</code> variant active for all packages that have it.</p> <p>Note</p> <p>It is good practice to have a single root spec in an environment, and to define constraint on packages or specific dependencies in the <code>packages:</code> field.</p> <p>Tip</p> <p>To create an environment you can also use <code>spack env create SPACK_ENV_FOLDER</code> and edit the <code>spack.yaml</code> file with <code>spack -e SPACK_ENV_FOLDER config edit</code>. Alternatively, you can use <code>spack -e SPACK_ENV_FOLDER add &lt;spec&gt;</code> to add root specs to the environment and <code>spack -e config add &lt;config&gt;</code> to add configutations to the environment. </p> <p>Example</p> <p>An example of creating an environment for building Quantum ESPRESSO: <pre><code>spack env create qe-env\nspack -e qe-env  add quantum-espresso%nvhpc +cuda               # Add spec for Quantum ESPRESSO\nspack -e qe-env config add packages:all:prefer:cuda_arch=90     # Add configuration for all packages \n</code></pre></p>"},{"location":"uenv-compilation-spack/#building","title":"Building","text":"<p>After defining the environment above, you can concretize it:</p> <pre><code>spack -e SPACK_ENV_FOLDER concretize -f\n</code></pre> <p>The result of the concretization will be printed on screen. Packages marked with <code>-</code> are packages that will be freshly installed in your local Spack instance. Packages marked as <code>[^]</code> (upstream) are packages taken directly from the uenv (which is being used as upstream Spack instance). You should see many packages marked as <code>[^]</code>, which are being re-used from the uenv. <code>[e]</code> (external) are external packages that are already installed in the system (and are defined in the system configuration of the system for which the uenv is built). Finally, packages marked as <code>[+]</code> are packages that are already installed in your local Spack instances.</p> <p>Using the uenv as an upstream Spack instance will greatly speed up compilation, since Spack will have to build only a small subset of packages.</p> <p>You can finally build everything in the concretized environment:</p> <pre><code>spack -e SPACK_ENV_FOLDER install\n</code></pre>"},{"location":"uenv-compilation-spack/#developing-with-spack-build-manually","title":"Developing with Spack (build manually)","text":"<p>In addition to wanting to build a different configuration of a package as described above, you might want to build your own development version of the software from source. Let's assume you want to develop Quantum ESPRESSO, with GPU-aware MPI:</p> <pre><code>quantum-espresso@7.3.1 %nvhpc +libxc +cuda cuda_arch=90 +gpumpi\n</code></pre>"},{"location":"uenv-compilation-spack/#spack-environment-and-building-dependencies","title":"Spack Environment and Building Dependencies","text":"<p>As described above, you can define a Spack environment describing the version of the package you want to build and the constraints on the dependencies. After concretizing the environment with</p> <pre><code>spack -e SPACK_ENV_FOLDER concretize -f\n</code></pre> <p>you can tell Spack to only install the dependencies, since you want to build the root spec manually:</p> <pre><code>spack -e SPACK_ENV_FOLDER install --only=dependencies\n</code></pre>"},{"location":"uenv-compilation-spack/#building-the-root-spec-manually","title":"Building the root spec manually","text":"<p>Finally, you are ready to build the root spec manually. With Spack you can get a shell within the build environment as follows:</p> <pre><code>spack -e SPACK_ENV_FOLDER build-env quantum-espresso -- bash\n</code></pre> <p>where <code>quantum-espresso</code> denotes the root spec. Since there is only one such spec, there is no need to explicitly write out the version nor the variants.</p> <p>Within the build environment, the software can be built using the provided build system. Quantum ESPRESSO uses CMake, therefore you can simply do the following:</p> <pre><code>mkdir build &amp;&amp; cd build\n\ncmake \\\n    -GNinja\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_ENABLE_MPI_GPU_AWARE=ON \\\n    -DQE_ENABLE_LIBXC=ON \\\n    ..\n\nninja -j 32\n</code></pre>"},{"location":"uenv-compilation-spack/#developing-with-spack-build-with-spack","title":"Developing with Spack (build with Spack)","text":"<p>Spack already knows how to build Quantum ESPRESSO with CMake, therefore you could use Spack to build your development version for you.</p> <p>Warning</p> <p>Changes to CMake might require changes to the Quantum ESPRESSO Spack package.</p>"},{"location":"uenv-compilation-spack/#spack-environment_1","title":"Spack Environment","text":"<p>You can create a Spack environment as suggested above:</p> <pre><code>spack env create qe-dev-env\nspack -e qe-dev-env add quantum-espresso%nvhpc +libxc +gpumpi\nspack -e $SCRATCH/qe-env config add packages:all:prefer:cuda_arch=90\n</code></pre> <p>In addition to adding Quantum ESPRESSO as a root spec, you have to tell Spack where to find the source code (and which version/branch it corresponds to). You can use the following command:</p> <pre><code>spack -e qe-dev-env develop -p PATH_TO_QE_SOURCE_CODE quantum-espresso@=develop\n</code></pre> <p>After concretizing the environment with</p> <pre><code>spack -e SPACK_ENV_FOLDER concretize -f\n</code></pre> <p>you can tell Spack to install everything, including Quantum ESPRESSO using the source code in <code>PATH_TO_QE_SOURCE_CODE</code>:</p> <pre><code>spack -e SPACK_ENV_FOLDER install\n</code></pre>"},{"location":"uenv-compilation-spack/#known-limitations","title":"Known Limitations","text":"<p>Warning</p> <p>Swapping the upstream Spack instance by loading different uenvs might lead to surprising inconsistencies in the Spack database. If this happens, you can uninstall everything from your local Spack instance with <code>spack uninstall --all</code> and clean up with <code>spack clean --all</code>. To avoid this problem, you can also work with multiple local Spack instances (one for each uenv).</p>"},{"location":"uenv-cp2k/","title":"CP2K","text":"<p>CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems.</p> <p>CP2K provides a general framework for different modeling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, \u2026), and classical force fields (AMBER, CHARMM, \u2026). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimization, and transition state optimization using NEB or dimer method. See CP2K Features for a detailed overview.</p>"},{"location":"uenv-cp2k/#running","title":"Running","text":"<pre><code>uenv start &lt;CP2K_UENV&gt;\nuenv view modules\nmodule load cp2k\n</code></pre> <p>or</p> <pre><code>uenv start &lt;CP2K_UENV&gt;\nuenv view cp2k\n</code></pre> <p>Warning</p> <p>COSMA is built with GPU-aware MPI. Make sure to set <code>MPICH_GPU_SUPPORT_ENABLED=1</code> when running CP2K.</p>"},{"location":"uenv-cp2k/#building-from-source","title":"Building from source","text":"<p>The CP2K <code>uenv</code> provides all the dependencies required to build CP2K from source, with several optional features enabled. You can follow these steps to build CP2K from source:</p> <pre><code># Start uenv and load develop view\nuenv start &lt;CP2K_UENV&gt;\nuenv view develop\n\n# cd to CP2K source directory\ncd &lt;PATH_TO_CP2K_SOURCE&gt;\n\n# CMake\nmkdir build &amp;&amp; cd build\nCC=mpicc CXX=mpic++ FC=mpifort cmake \\\n    -GNinja \\\n    -DCP2K_ENABLE_REGTESTS=ON \\\n    -DCP2K_USE_LIBXC=ON \\\n    -DCP2K_USE_LIBINT2=ON \\\n    -DCP2K_USE_SPGLIB=ON \\\n    -DCP2K_USE_ELPA=ON \\\n    -DCP2K_USE_SPLA=ON \\\n    -DCP2K_USE_SIRIUS=ON \\\n    -DCP2K_USE_COSMA=ON \\\n    -DCP2K_USE_PLUMED=ON \\\n    -DCP2K_USE_ACCEL=CUDA -DCP2K_WITH_GPU=H100 \\\n    ..\n\nninja -j 32\n</code></pre> <p>Note</p> <p><code>cp2k@2024.1</code> does not support compiling for <code>cuda_arch=90</code>. Use <code>-DCP2K_WITH_GPU=A100</code> instead.</p> <p>Note</p> <p>On <code>x86</code> we deploy with <code>intel-oneapi-mkl</code> and <code>libxsmm</code>. Add <code>-DCP2K_SCALAPACK_VENDOR=MKL</code> to the CMake invocation to find MKL, and optionally <code>-DCP2K_USE_LIBXSMM=ON</code> to use <code>libxsmm</code>.</p> <p>See manual.cp2k.org/CMake for more details.</p>"},{"location":"uenv-gromacs/","title":"GROMACS","text":"<p>GROMACS (GROningen Machine for Chemical Simulations) is a versatile and widely-used open source package to perform molecular dynamics, i.e. simulate the Newtonian equations of motion for systems with hundreds to millions of particles. It is primarily designed for biochemical molecules like proteins, lipids and nucleic acids that have a lot of complicated bonded interactions, but since GROMACS is extremely fast at calculating the nonbonded interactions (that usually dominate simulations) many groups are also using it for research on non-biological systems, e.g. polymers.</p> <p>Licensing Terms and Conditions</p> <p>GROMACS is a joint effort, with contributions from developers around the world: users agree to acknowledge use of GROMACS in any reports or publications of results obtained with the Software (see GROMACS Homepage for details).</p>"},{"location":"uenv-gromacs/#alps-gh200","title":"ALPS (GH200)","text":""},{"location":"uenv-gromacs/#setup","title":"Setup","text":"<p>On ALPS, we provide pre-built user environments containing GROMACS alongside all the required dependencies for the GH200 hardware setup. To access the <code>gmx_mpi</code> executable, we do the following:</p> <pre><code># list the gromacs images and versions available on the system\nuenv image find gromacs\n\nuenv image pull gromacs/2024:v1\nuenv start gromacs/2024:v1\n\nuenv view gromacs     # load the gromacs view\ngmx_mpi --version     # check GROMACS version\n</code></pre> <p>The images also provide two alternative views, namely <code>plumed</code> and <code>develop</code>. After starting the pulled image using\u00a0<code>uenv start ...</code>, one may do the following see the available views.</p> <pre><code>$ uenv status\n/user-environment:gromacs-gh200\n  GPU-optimised GROMACS with and without PLUMED, and the toolchain to build your own GROMACS.\n  modules: no modules available\n  views:\n    develop\n    gromacs\n    plumed\n</code></pre> <p>The <code>develop</code> view has all the required dependencies or GROMACS without the program itself. This is meant for those users who want to use a customized variant of GROMACS for their simulation which they build from source. This view makes it convenient for users as it provides the required compilers (GCC 12), CMake, CUDA, hwloc, Cray MPICH, among many others which their GROMACS can use during build and installation. Users must enable this view each time they want to use their custom GROMACS installation.</p> <p>The <code>plumed</code> view contains GROMACS 2022.5 (older version) with PLUMED 2.9.0. This is due to the compatibility requirements of PLUMED. CSCS will periodically update these user environment images to feature newer versions as they are made available.</p> <p>The <code>gromacs</code> view contains the newest GROMACS 2024.1 that has been configured and tested for the highest performance on the Grace-Hopper nodes.</p>"},{"location":"uenv-gromacs/#how-to-run","title":"How to Run","text":"<p>To start a job, 2 bash scripts are required: a standard SLURM submission script, and a wrapper to start the CUDA MPS daemon (in order to have multiple MPI ranks per GPU).</p> <p>The CUDA MPS wrapper here: <pre><code>#!/bin/bash\n# Example mps-wrapper.sh usage:\n# &gt; srun [...] mps-wrapper.sh -- &lt;cmd&gt;\n\nTEMP=$(getopt -o '' -- \"$@\")\neval set -- \"$TEMP\"\n\n# Now go through all the options\nwhile true; do\n    case \"$1\" in\n        --)\n            shift\n            break\n            ;;\n        *)\n            echo \"Internal error! $1\"\n            exit 1\n            ;;\n    esac\ndone\n\nset -u\n\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log\n# Launch MPS from a single rank per node\nif [ $SLURM_LOCALID -eq 0 ]; then\n    CUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\nfi\n# Wait for MPS to start sleep 5\nsleep 5\n\nexec \"$@\"\n</code></pre></p> <p>The wrapper script above can be made executable with\u00a0<code>chmod +x mps-wrapper.sh</code>. The SLURM submission script can be adapted from the template below to use the application and the <code>mps-wrapper.sh</code> in conjunction.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=\"JOB NAME\"\n#SBATCH --nodes=1             # number of GH200 nodes with each node having 4 CPU+GPU\n#SBATCH --ntasks-per-node=8   # 8 MPI ranks per node\n#SBATCH --cpus-per-task 32    # 32 OMP threads per MPI rank\n#SBATCH --account=ACCOUNT\n#SBATCH --hint=nomultithread  \n\nexport MPICH_GPU_SUPPORT_ENABLED=1\n\nexport GMX_GPU_DD_COMMS=true\nexport GMX_GPU_PME_PP_COMMS=true\nexport GMX_FORCE_UPDATE_DEFAULT_GPU=true\nexport GMX_ENABLE_DIRECT_GPU_COMM=1\nexport GMX_FORCE_GPU_AWARE_MPI=1\n\nsrun ./mps-wrapper.sh -- gmx_mpi mdrun -s input.tpr -ntomp 32 -bonded gpu -nb gpu -pme gpu -pin on -v -noconfout -dlb yes -nstlist 300 -gpu_id 0123 -npme 1 -nsteps 10000 -update gpu\n</code></pre> <p>This can be run using <code>sbatch launch.sbatch</code> on the login node with the user environment loaded.</p> <p>This submission script is only representative. Users must run their input files with a range of parameters to find an optimal set for the production runs. Some hints for this exploration below:</p> <p>Configuration Hints</p> <ul> <li>Each Grace CPU has 72 cores, but a small number of them are used for the underlying processes such as runtime daemons. So all 72 cores are not available for compute. To be safe, do not exceed more than 64 OpenMP threads on a single CPU even if it leads to a handful of cores idling.</li> <li>Each node has 4 Grace CPUs and 4 Hopper GPUs. When running 8 MPI ranks (meaning two per CPU), keep in mind to not ask for more than 32 OpenMP threads per rank. That way no more than 64 threads will be running on a single CPU.</li> <li>Try running both 64 OMP threads x 1 MPI rank and 32 OMP threads x 2 MPI ranks configurations for the test problems and pick the one giving better performance. While using multiple GPUs, the latter can be faster by 5-10%.</li> <li><code>-update gpu</code>\u00a0 may not be possible for problems that require constraints on all atoms. In such cases, the update (integration) step will be performed on the CPU. This can lead to performance loss of at least 10% on a single GPU. Due to the overheads of additional data transfers on each step, this will also lead to lower scaling performance on multiple GPUs.</li> <li>When running on a single GPU, one can either configure the simulation with 1-2 MPI ranks with <code>-gpu_id</code>\u00a0 as <code>0</code> , or try running the simulation with a small number of parameters and let GROMACS run with defaults/inferred parameters with a command like the following in the SLURM script: <code>srun ./mps-wrapper.sh -- gmx_mpi mdrun -s input.tpr -ntomp 64</code> </li> <li>Given the compute throughput of each Grace-Hopper module (single CPU+GPU), for smaller-sized problems, it is possible that a single-GPU run is the fastest. This may happen when the overheads of communication and orchestration exceed the benefits of parallelism across multiple GPUs. In our test cases, a single Grace-Hopper module has consistently shown a 6-8x performance speedup over a single node on Piz Daint.</li> </ul> <p>Known Performance/Scaling Issues</p> <ul> <li>The current build of GROMACS on our system allows only one MPI rank to be dedicated for PME with <code>-nmpe 1</code>. This becomes a serious performance limitation for larger systems where the non-PME ranks finish their work before the PME rank leading to unwanted load imbalances across ranks. This limitation is targeted to be fixed in the subsequent releases of our builds of user environments.</li> <li>The above problem is especially critical for large problem sizes (1+ million atom systems) but is far less apparent in small and medium sized runs.</li> <li>If the problem allows the integration step to take place on the GPU with <code>-update gpu</code>, that can lead to significant performance and scaling gains as it allows an even greater part of the computations to take place on the GPU.</li> <li>SLURM and CUDA MPS configurations are being explored to extend the simulation beyond a single compute node (of 4 CPUs+GPUs). Documentation will be updated once scaling across nodes is reliably reproduced. As of now, simulations are recommended to be contained to a single node.</li> </ul>"},{"location":"uenv-lammps/","title":"LAMMPS","text":"<p>LAMMPS is a classical molecular dynamics code with a focus on materials modeling. It's an acronym for Large-scale Atomic/Molecular Massively Parallel Simulator. LAMMPS has potentials for solid-state materials (metals, semiconductors) and soft matter (biomolecules, polymers) and coarse-grained or mesoscopic systems. It can be used to model atoms or, more generically, as a parallel particle simulator at the atomic, meso, or continuum scale. See LAMMPS Features for a detailed overview.</p>"},{"location":"uenv-lammps/#running","title":"Running","text":"<pre><code>uenv start &lt;LAMMPS_UENV&gt;\nuenv view lammps\n</code></pre> <p>Warning</p> <p>LAMMPS is built with GPU-aware MPI. Make sure to set <code>MPICH_GPU_SUPPORT_ENABLED=1</code> when running LAMMPS.</p>"},{"location":"uenv-lammps/#building-from-source","title":"Building from source","text":"<p>The LAMMPS <code>uenv</code> provides all the dependencies required to build LAMMPS from source, including kokkos. You can follow these steps to build LAMMPS from source:</p> <pre><code># Start uenv and load develop view\nuenv start &lt;LAMMPS_UENV&gt;\nuenv view develop\n\n# cd to LAMMPS source directory\ncd &lt;PATH_TO_LAMMPS_SOURCE&gt;\n\n# CMake\nmkdir build &amp;&amp; cd build\ncmake -C ../cmake/presets/kokkos-cuda.cmake ../cmake/  -DKokkos_ENABLE_IMPL_CUDA_MALLOC_ASYNC=OFF -DKokkos_ARCH_NATIVE=yes -DKokkos_ARCH_HOPPER90=yes\n\ncmake --build . --parallel 32\n</code></pre>"},{"location":"uenv-linaro-forge/","title":"Linaro Forge (DDT) debugger","text":"<p>Linaro Forge (formerly known as DDT) allows source-level debugging of Fortran, C, C++ and Python codes. It can be used for debugging serial, multi-threaded (OpenMP), multi-process (MPI) and accelerated (Cuda, OpenACC) programs running on research and production systems, including CSCS Alps system. It can be executed either as a graphical user interface or from the command-line.</p>"},{"location":"uenv-linaro-forge/#usage-notes","title":"Usage notes","text":"<p>The uenv is named <code>linaro-forge</code>, and the available versions on a cluster can be determined using the <code>uenv image find</code> command, for example: <pre><code>&gt; uenv image find linaro-forge\nuenv/version:tag                        uarch date       id               size\nlinaro-forge/23.1.2:latest              gh200 2024-04-10 ea67dbb33801c7c3 342MB\n</code></pre></p> <p>The linaro tools are configured to be mounted in the <code>/user-tools</code> path so that they can be used alongside application and development uenv mounted at <code>user-environment</code>.</p> sidecarstandalone <p>When using alongside another uenv, start a uenv session with both uenv with <code>linaro-forge</code> after the main uenv, to mount the images at the respective <code>/user-environment</code> and <code>/user-tools</code> locations:</p> <pre><code>uenv start prgenv-gnu/24.2:v3 linaro-forge/23.1.2\n\n# test that everything has been mounted correctly\n# (will give warnings if there are problems)\nuenv status\n\nuenv view prgenv-gnu:default\nsource /user-tools/activate\n\n# check that ddt is in the path\nddt --version\n</code></pre> <p>The <code>/user-tools/activate</code> script will make the forge executables available in your environment, and must be run after any other uenv view command.</p> <p>When using the uenv with no other environment mounted, you will need to explicitly set the <code>/user-tools</code> mount point:</p> <pre><code>uenv start linaro-forge/23.1.2:/user-tools\n\nsource /user-tools/activate\n\n# check that ddt is in the path\nddt --version\n</code></pre> <p>The <code>/user-tools/activate</code> script will make the forge executables available in your environment.</p>"},{"location":"uenv-linaro-forge/#getting-started","title":"Getting Started","text":"<p>In order to debug your code on Alps, you need to:</p> <ol> <li>pull the linaro-forge uenv on the target Alps vCluster</li> <li>install the Forge/DDT client on your laptop</li> <li>build an executable with debug flags</li> <li>launch a job with the debugger on Alps.</li> </ol>"},{"location":"uenv-linaro-forge/#pull-the-linaro-forge-uenv-on-the-alps-cluster","title":"Pull the Linaro Forge uenv on the Alps cluster","text":"<p>The first step is to download the latest version of linaro-forge that is available on the cluster. First, SSH into the target system, then use the <code>uenv image find</code> command to list the available versions on the system:</p> <pre><code>&gt; uenv image find linaro-forge\nuenv/version:tag                        uarch date       id               size\nlinaro-forge/23.1.2:latest              gh200 2024-04-10 ea67dbb33801c7c3 342MB\n</code></pre> <p>In this example, there is a single version available. Next we pull the image so that it is available locally. <pre><code>&gt; uenv image pull linaro-forge/23.1.2:latest\n</code></pre></p> <p>It will take a few seconds to download the image. Once complete, check that it was downloaded using the <code>uenv image ls</code> command:</p> <pre><code>&gt; uenv image ls linaro-forge\nuenv/version:tag                        uarch date       id               size\nlinaro-forge/23.1.2:latest              gh200 2024-04-05 ea67dbb33801c7c3 342MB\n</code></pre>"},{"location":"uenv-linaro-forge/#install-the-client-on-your-laptop","title":"Install the client on your laptop","text":"<p>We recommend installing the desktop client on your local workstation/laptop. It can be configured to connect with the debug jobs running on Alps, offering a better user experience compared running remotely with X11 forwarding. The client can be downloaded for a selection of operating systems, via the link above.</p> <p>Once installed, the client needs to be configured to connect to the vCluster on which you are working. First, start the client on your laptop.</p> LinuxMacOS <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location.</p> <pre><code>$HOME/linaro/forge/23.0.1/bin/ddt\n</code></pre> <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location.</p> <pre><code>open /Applications/Linaro\\ Forge\\ Client\\ 23.0.1.app/\n</code></pre> <p>Next, configure a connection to the target system. Open the Remote Launch menu and click on configure then Add. Examples of the settings are below.</p> EigerSantis Field Value Connection <code>eiger</code> Host Name <code>bsmith@ela.cscs.ch bsmith@eiger.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/23.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>$HOME/.ssh/cscs-key</code> Field Value Connection <code>santis</code> Host Name <code>bsmith@ela.cscs.ch bsmith@santis.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/23.1.2:/user-tools -- /user-tools/env/forge/</code> Private Key <code>$HOME/.ssh/cscs-key</code> <p>Some notes on the examples above:</p> <ul> <li>SSH Forwarding via <code>ela.cscs.ch</code> is used to access the cluster.</li> <li>the replace the username <code>bsmith</code> with your CSCS user name that you would normally use to open an SSH connection to CSCS.</li> <li>the Remote Installation Path is a little bit more complicated than</li> <li>the private keys should be the ones generated for CSCS MFA, and this field does not need to be set if you have added the key to your SSH agent.</li> </ul> <p>Once configured, test and save the configuration:</p> <ol> <li>check whether the concfiguration is correct, click <code>Test Remote Launch</code>.</li> <li>Click on <code>ok</code> and <code>close</code> to save the configuration.</li> <li>You can now connect by going to <code>Remote Launch</code> and choose the <code>Alps</code> entry. If the client fails to connect, look at the message, check your ssh configuration and make sure you can ssh without the client.</li> </ol>"},{"location":"uenv-linaro-forge/#setup-the-environment","title":"Setup the environment","text":""},{"location":"uenv-linaro-forge/#build-with-debug-flags","title":"Build with debug flags","text":"<p>Once the uenv is loaded and activated, the program to debug must be compiled with the <code>-g</code> (for cpu) and <code>-G</code> (for gpu) debugging flags. For example, let's build a cuda code with  a user environment:</p> <pre><code>uenv start prgenv-gnu:24.2:v2\nuenv view default\n\n# download the source code\ngit clone https://github.com/sekelle/octree-miniapp.git\ncd o\n\n\n# build the application\nmake -C octree-miniapp.git/\n</code></pre>"},{"location":"uenv-linaro-forge/#launch-the-code-with-the-debugger","title":"Launch the code with the debugger","text":"<p>To use the DDT client with uenv, it must be launched in <code>Manual Launch</code> mode (assuming that it is connected to Alps via <code>Remote Launch</code>):</p> Note <p>the steps below do not manually launch - instead they directly launch using <code>ddt --connect srun ...</code> on the target cluster.</p> on laptopon Alps <p>Start DDT, and connect to the target cluster using the drop down menu for Remote Launch.</p> <p>Then wait for the job to start (see the \"on Alps\" tab).</p> <p>log into the system and launch with the srun command:</p> <pre><code># start a session with both the PE used to build your application\n# and the linaro-forge uenv mounted\nuenv start prgenv-gnu/24.2 linaro-forge/23.1.2\nddt --connect srun -n2 -N2 ./a.out\n</code></pre> <p>Notes on using specific systems:</p> santis <p>Warning</p> <p>Because Santis is not connected to the internet, some environment variables need to be set so that it can connect to the license server.</p> <pre><code>export https_proxy=proxy.cscs.ch:8080\nexport http_proxy=proxy.cscs.ch:8080\nexport no_proxy=\".local, .cscs.ch, localhost, 148.187.0.0/16, 10.0.0.0/8, 172.16.0.0/12\"\n</code></pre> default value of <code>http_proxy</code> <p>By default the <code>https_proxy</code> and <code>http_proxy</code> variables are set to <code>http://proxy.cscs.ch:8080</code>, as the transport is required for some other services to work. You will have to set them for a debugging session.</p> <p>This screenshot shows a debugging session on 12 gpus:</p> <p></p>"},{"location":"uenv-namd/","title":"NAMD","text":"<p>NAMD is a parallel molecular dynamics code based on Charm++, designed for high-performance simulation of large biomolecular systems.</p> <p>Licensing Terms and Conditions</p> <p>NAMD is distributed free of charge for research purposes only and not for commercial use: users must agree to NAMD license in order to use it at CSCS. Users agree to acknowledge use of NAMD in any reports or publications of results obtained with the Software (see NAMD Homepage for details).</p>"},{"location":"uenv-namd/#single-node-build","title":"Single-node build","text":"<p>The single-node build works on a single node and benefits from the new GPU-resident mode (see NAMD 3.0b6 GPU-Resident benchmarking results for more details). The single-node build provides the following views:</p> <ul> <li><code>namd-single-node</code> (standard view, with NAMD)</li> <li><code>develop-single-node</code> (development view, without NAMD)</li> </ul>"},{"location":"uenv-namd/#building-from-source","title":"Building from source","text":"<p>TCL Version</p> <p>According to the NAMD 3.0 release notes, TCL <code>8.6</code> is required. However, the source code for the <code>3.0</code> release still contains hard-coded flags for TCL <code>8.5</code>. The UENV provides <code>tcl@8.6</code>, therefore you need to manually modify NAMD 3.0's <code>arch/Linux-ARM64.tcl</code> file as follows: change <code>-ltcl8.5</code> to <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <p>The NAMD <code>uenv</code> provides all the dependencies required to build NAMD from source. You can follow these steps to build NAMD from source:</p> <pre><code>export DEV_VIEW_NAME=\"develop-single-node\"\n\n# Start uenv and load develop view\nuenv start &lt;NAMD_UENV&gt;\nuenv view ${DEV_VIEW_NAME}\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd &lt;PATH_TO_NAMD_SOURCE&gt;\n\n# ~~~~~\n# Modify the \"&lt;PATH_TO_NAMD_SOURCE&gt;/arch/Linux-ARM64.tcl\" file now!\n# Change \"-ltcl8.5\" with \"-ltcl8.6\" in the definition of the \"TCLLIB\" variable\n# ~~~~~\n\n# Build bundled Charm++\ntar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\n./build charm++ multicore-linux-arm8 gcc --with-production --enable-tracing -j 32\n\n# Configure NAMD build for GPU\ncd .. \n./config Linux-ARM64-g++.cuda \\\n    --charm-arch multicore-linux-arm8-gcc --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH} \\\n    --cuda-gencode arch=compute_90,code=sm_90 --with-single-node-cuda --with-cuda --cuda-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++.cuda &amp;&amp; make -j 32\n\n# !!! BEGIN OPTIONAL !!!\n# Configure NAMD build for CPU\ncd ..\n./config Linux-ARM64-g++ \\\n    --charm-arch multicore-linux-arm8-gcc --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++ &amp;&amp; make -j 32\n# !!! END OPTIONAL !!!\n\ncd ..\nexport LD_LIBRARY_PATH=${DEV_VIEW_PATH}/lib/\n\n# Run NAMD (GPU version)\nLinux-ARM64-g++.cuda/namd3 &lt;NAMD_OPTIONS&gt;\n\n# !!! BEGIN OPTIONAL !!!\n# Run NAMD (CPU version)\nLinux-ARM64-g++/namd3 &lt;NAMD_OPTIONS&gt;\n# !!! END OPTIONAL !!!\n</code></pre> <p>The optional section provides instructions on how to build a CPU-only build, should you need it (for constant pH MD simulations, for example).</p>"},{"location":"uenv-namd/#multi-node-build","title":"Multi-node build","text":"<p>The multi-node build works on multiple nodes and it is based on Charm++ MPI backend. The multi-node build provides the following views:</p> <ul> <li><code>namd</code></li> <li><code>develop</code> (development view, without NAMD)</li> </ul> <p>GPU-resident mode</p> <p>The multi-node build based on Charm++ MPI backend can't take advantage of the new GPU-resident mode. Unless you require the multi-node build or you can prove it is faster for your use case, we recommend using the single-node build with the GPU-resident mode.</p>"},{"location":"uenv-namd/#building-from-source_1","title":"Building from source","text":"<p>TCL Version</p> <p>According to the NAMD 3.0 release notes, TCL <code>8.6</code> is required. However, the source code for the <code>3.0</code> release still contains hard-coded flags for TCL <code>8.5</code>. The UENV provides <code>tcl@8.6</code>, therefore you need to manually modify NAMD 3.0's <code>arch/Linux-ARM64.tcl</code> file as follows: change <code>-ltcl8.5</code> to <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <p>The NAMD <code>uenv</code> provides all the dependencies required to build NAMD from source. You can follow these steps to build NAMD from source:</p> <pre><code>export DEV_VIEW_NAME=\"develop\"\n\n# Start uenv and load develop view\nuenv start &lt;NAMD_UENV&gt;\nuenv view ${DEV_VIEW_NAME}\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd &lt;PATH_TO_NAMD_SOURCE&gt;\n\n# ~~~~~\n# Modify the \"arch/Linux-ARM64.tcl\" file now!\n# Change \"-ltcl8.5\" with \"-ltcl8.6\" in the definition of the \"TCLLIB\" variable\n# ~~~~~\n\n# Build bundled Charm++\ntar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\nenv MPICXX=mpicxx ./build charm++ mpi-linux-arm8 smp --with-production -j 32\n\n# Configure NAMD build for GPU\ncd .. \n./config Linux-ARM64-g++.cuda \\\n    --charm-arch mpi-linux-arm8-smp --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH} \\\n    --cuda-gencode arch=compute_90,code=sm_90 --with-single-node-cuda --with-cuda --cuda-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++.cuda &amp;&amp; make -j 32\n\n# !!! BEGIN OPTIONAL !!!\n# Configure NAMD build for CPU\ncd ..\n./config Linux-ARM64-g++ \\\n    --charm-arch mpi-linux-arm8-smp --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++ &amp;&amp; make -j 32\n# !!! END OPTIONAL !!!\n\ncd ..\nexport LD_LIBRARY_PATH=${DEV_VIEW_PATH}/lib/\n\n# Run NAMD (GPU version)\nLinux-ARM64-g++.cuda/namd3 &lt;NAMD_OPTIONS&gt;\n\n# !!! BEGIN OPTIONAL !!!\n# Run NAMD (CPU version)\nLinux-ARM64-g++/namd3 &lt;NAMD_OPTIONS&gt;\n# !!! END OPTIONAL !!!\n</code></pre> <p>The optional section provides instructions on how to build a CPU-only build, should you need it (for constant pH MD simulations, for example).</p>"},{"location":"uenv-namd/#useful-links","title":"Useful Links","text":"<ul> <li>NAMD Spack package</li> <li>NAMD Tutorials</li> <li>Charm++ Spack package</li> <li>Running Charm++ Programs</li> <li>What you should know about NAMD and Charm++ but were hoping to ignore by J. C. Phillips</li> </ul>"},{"location":"uenv-overview/","title":"Supported UENV on Alps","text":"<p>On Alps CSCS provides supported uenv that serve different application and development environment use cases.</p> <p>The list of supported uenv on a specific Alps system (<code>eiger</code>, <code>clariden</code>, <code>santis</code> etc) is documented in the CSCS knowledge base. Please read the knowledge base documentaiton for more details on specific systems.</p> <p>These pages here present detailed documentation for each of the officially supported UENV.</p> <ul> <li>CP2K is provided by <code>cp2k</code></li> <li>GROMACS is provided by <code>gromacs</code></li> <li>Linaro Forge is provided by <code>linaro-forge</code></li> <li>NAMD is provided by <code>namd</code></li> <li>Quantum Espresso is provided by <code>quantumespresso</code></li> </ul>"},{"location":"uenv-prgenv-gnu/","title":"prgenv-gnu uenv","text":"<p>Provides a small set of tools and libraries built around the GNU compiler toolchain.</p> <p>It provides the GCC compilers (gcc, g++ and gfortran), MPI (cray-mpich), Python, cuda (on systems with NVIDIA GPUs).</p> <p>The following packages are provided:</p> <ul> <li><code>aws-ofi-nccl</code></li> <li><code>cmake</code></li> <li><code>cray-mpich</code><ul> <li>built with <code>cuda</code> support on systems with NVIDIA GPUs</li> </ul> </li> <li><code>cuda</code><ul> <li>only on systems with NVIDIA GPUs</li> </ul> </li> <li><code>fftw</code></li> <li><code>fmt</code></li> <li><code>gcc</code></li> <li><code>hdf5</code></li> <li><code>libtree</code></li> <li><code>meson</code></li> <li><code>nccl-tests</code></li> <li><code>nccl</code></li> <li><code>ninja</code></li> <li><code>openblas</code><ul> <li>built with OpenMP support</li> </ul> </li> <li><code>osu-micro-benchmarks</code></li> <li><code>python</code><ul> <li>a recent version of python 3</li> </ul> </li> </ul>"},{"location":"uenv-prgenv-gnu/#how-to-use","title":"How to use","text":"<p>The environment is designed as a fairly minimal set of </p> <p>There are three ways to access the software provided by prgenv-gnu, once it has been started.</p> viewsmodulesspack <p>The simplest way to get started is to use the file system view. A single view is provided:</p> <ul> <li>before v24.7: the <code>default</code> view</li> <li>since v24.7: the <code>develop</code> view</li> </ul> <pre><code># set when starting the uenv\nuenv start --view=develop prgenv-gnu/24.7:v1\n\n# set after starting\n# NOTE: this method will be deprecated\nuenv start prgenv-gnu/24.7:v1\nuenv view develop\n\n# example: the python executable provided by the uenv will be available\nwhich python\n/user-environment/env/default/bin/python\n\n# example: the python version is more recent that the 3.6 version in /usr/bin\npython --version \nPython 3.12.1\n</code></pre> <p>The uenv provides modules for all of the software packages. The modules are not available by default when a uenv starts, and have to be enabled.</p> <pre><code># with v4 of uenv:\nuenv start prgenv-gnu/24.7\nuenv modules use\n\n# with v5 of uenv:\n\n# method 1: enable modules when the uenv is started\nuenv start --view=modules prgenv-gnu/24.7\n\n# method 2: enable modules after the uenv has started\nuenv start prgenv-gnu/24.7\nuenv view modules\n</code></pre> <p>To use Spack, you can check the guide for using Spack with uenv.</p> <p>Note</p> <p>If using the most recent release of uenv and a compatible uenv, load the <code>spack</code> view:</p> <pre><code># start the uenv with the spack view\n# note: the version might differ from the one in this example\nuenv start --view=spack prgenv-gnu/24.7:v1\n</code></pre> <p>Loading the <code>spack</code> view sets the following environment variables (with example values):</p> <pre><code>UENV_SPACK_CONFIG_PATH  /user-environment/config\nUENV_SPACK_URL          https://github.com/spack/spack.git\nUENV_SPACK_COMMIT       b5fe93fee1eec46a0750bd340198bffcb92ff9eec\n</code></pre>"},{"location":"uenv-prgenv-gnu/#platform-specific-hints","title":"platform specific hints","text":"gh200multicore <p>The version of MPI (cray-mpich) provided on Grace Hopper systems (Todi at the time of writing) supports GPU-direct or GPU aware communication, whereby pointers to GPU buffers can be passed directly to MPI calls. No special steps have to be taken to compile your code, however the following environment variable must be set to run an application that uses GPU pointers:</p> <pre><code>export MPICH_GPU_SUPPORT_ENABLED=1\n</code></pre> <p>There are no platform-specific notes for multicore.</p>"},{"location":"uenv-qe/","title":"Quantum ESPRESSO","text":"<p>https://www.quantum-espresso.org</p> <p>An environment that provides the latest version of Quantum ESPRESSO, along with the libraries and tools required to build a different or custom version of Quantum ESPRESSO. At the moment a GPU-build environment is provided without a ScaLAPACK.</p> <p>The following environment views are provided:</p> GH200A100 <ul> <li>default : QuantumESPRESSO/7.3.1 itself + dependencies</li> <li>develop : only dependencies</li> </ul> <p>The following modules are provided:</p> <ul> <li>cmake/3.27.7</li> <li>nvhpc/24.1</li> <li>quantum-espresso/7.3.1</li> <li>cray-mpich/8.1.29</li> <li>fftw/3.3.10</li> <li>git/2.42.0</li> <li>nvpl-lapack/0.2.0</li> <li>gcc/12.3.0</li> <li>libxc/6.2.2</li> <li>nvpl-blas/0.1.0</li> </ul> <ul> <li>default : QuantumESPRESSO/7.1 itself + dependencies</li> <li>develop : only dependencies</li> </ul> <p>The following modules are provided:</p> <ul> <li>cmake/3.26.3</li> <li>cray-mpich/8.1.25-nvhpc</li> <li>cuda/11.8.0</li> <li>fftw/3.3.10</li> <li>gcc/11.3.0</li> <li>libxc/5.2.3</li> <li>nvhpc/22.11</li> <li>openblas/0.3.23</li> <li>patchelf/0.17.2</li> <li>quantum-espresso/7.1</li> </ul>"},{"location":"uenv-qe/#building-a-custom-version","title":"Building a custom version","text":""},{"location":"uenv-qe/#using-modules","title":"Using modules","text":"GH200A100 <pre><code>uenv start quantumespresso/v7.3.1\nuenv modules use\nmodule load cmake \\\n    fftw \\\n    nvhpc \\\n    nvpl-lapack \\\n    nvpl-blas \\\n    cray-mpich \\\n    netlib-scalapack \\\n    libxc\n\nmkdir build &amp;&amp; cd build\nFC=mpif90 CXX=mpic++ CC=mpicc cmake .. \\\n    -DQE_ENABLE_MPI=ON \\\n    -DQE_ENABLE_OPENMP=ON \\\n    -DQE_ENABLE_SCALAPACK:BOOL=OFF \\\n    -DQE_ENABLE_LIBXC=ON \\\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_ENABLE_PROFILE_NVTX=ON \\\n    -DQE_CLOCK_SECONDS:BOOL=OFF \\\n    -DQE_ENABLE_MPI_GPU_AWARE:BOOL=OFF \\\n    -DQE_ENABLE_OPENACC=ON\nmake -j20\n</code></pre> <pre><code>uenv start quantumespresso/v7.1\nuenv modules use\nmodule load cmake \\\n    cray-mpich\n    cuda \\\n    fftw \\\n    gcc \\\n    libxc \\\n    nvhpc \\\n    openblas\nmkdir build &amp;&amp; cd build\nFC=mpif90 CXX=mpic++ CC=mpicc cmake .. \\\n    -DQE_ENABLE_MPI=ON \\\n    -DQE_ENABLE_OPENMP=ON \\\n    -DQE_ENABLE_SCALAPACK:BOOL=OFF \\\n    -DQE_ENABLE_LIBXC=ON \\\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_CLOCK_SECONDS:BOOL=OFF \\\n    -DQE_ENABLE_MPI_GPU_AWARE:BOOL=OFF \\\n    -DQE_ENABLE_OPENACC=ON\nmake -j20\n</code></pre>"},{"location":"uenv-qe/#using-spack","title":"Using spack","text":"<ol> <li> <p>Clone spack using the same version that has been used to build the uenv.  <pre><code>uenv start quantumespresso/v7.3.1\n# clone the same spack version as has been used to build the uenv\ngit clone -b $(jq -r .spack.commit /user-environment/meta/configure.json) $(jq -r .spack.repo /user-environment/meta/configure.json) $SCRATCH/spack\n</code></pre></p> </li> <li> <p>Activate spack with the uenv configured as upstream <pre><code># ensure spack is using the uenv as upstream repository (always required)\nexport SPACK_SYSTEM_CONFIG_PATH=/user-environment/config\n# active spack (always required)\n. $SCRATCH/spack/share/spack/setup-env.sh\n</code></pre></p> </li> <li> <p>Create an anonymous environment for QE <pre><code>spack env create -d $SCRATCH/qe-env\nspack -e $SCRATCH/qe-env add quantum-espresso%nvhpc +cuda\nspack -e $SCRATCH/qe-env config add packages:all:prefer:cuda_arch=90\nspack -e $SCRATCH/qe-env develop -p /path/to/your/QE-src quantum-espresso@=develop\nspack -e $SCRATCH/qe-env concretize -f\n</code></pre> Check the output of <code>spack concretize -f</code>. All dependencies should have been picked up from spack upstream, marked eiter by a green <code>[^]</code> or <code>[e]</code>. Next we create a local filesystem view, this instructs spack to create symlinks for binaries and libraries in a local directory <code>view</code>. <pre><code>spack -e $SCRATCH/qe-env env view enable view\nspack -e $SCRATCH/qe-env install\n</code></pre> To recompile QE after editing the source code re-run <code>spack -e $SCRATCH/qe-env install</code>.</p> </li> <li> <p>Run <code>pw.x</code> using the filesystem view generated in 3. <pre><code>uenv start quantumespresso/v7.3.1\nsrun [...] $SCRATCH/qe-env/view/bin/pw.x &lt; pw.in\n</code></pre> Note: The <code>pw.x</code> is linked to the uenv, it won't work without activating the uenv, also it will only work with the exact same version of the uenv. The physical installation path is in <code>$SCRATCH/spack</code>, deleting this directory will leave the anonymous spack environment created in 3. with dangling symlinks.</p> </li> </ol>"},{"location":"uenv-vasp/","title":"VASP","text":"<p>VASP (Vienna Ab initio Simulation Package) is a software package for performing ab initio quantum mechanical calculations.</p> <p>[!NOTE] VASP is only available to users with the appropriate license. Check the VASP website for licensing. Contact the CSCS service desk for license verification. Once verified, users are added to the <code>vasp6</code> group, which allows access to prebuilt images and the source code.</p>"},{"location":"uenv-vasp/#accessing-vasp-images","title":"Accessing VASP images","text":"<p>Failure</p> <p>Describe access to images. Not yet finalized.</p>"},{"location":"uenv-vasp/#usage","title":"Usage","text":"<p>The default build of vasp includes MPI, HDF5, Wannier90 and OpenACC (on GH200 and A100 architectures). Start the uenv and load the <code>vasp</code> view:</p> <p><pre><code>uenv start ${path_to_vasp_image}\nuenv view vasp\n</code></pre> The <code>vasp_std</code>, <code>vasp_gam</code> and <code>vasp_ncl</code> executables are now available for use.</p>"},{"location":"uenv-vasp/#build-from-source","title":"Build from source","text":"<p>Start the uenv and load the <code>develop</code> view: <pre><code>uenv start ${path_to_vasp_image}\nuenv view develop\n</code></pre> This will ensure that compiler executables are in <code>PATH</code>. The appropriate makefile from the <code>arch</code> directory in the vasp source tree should be selected and link / include paths changed to use the <code>/user-environment/env/develop</code> prefix, where all required dependencies can be found. For example on GH200 and A100 architectures, select <code>makefile.include.nvhpc_omp_acc</code> and change the gpu flags to match the architecture. In this case <code>-gpu=cc60,cc70,cc80,cuda11.0</code> to <code>-gpu=cc80,cc90,cuda12.2</code> (depending on the included cuda version). After changing all include / link paths, compile VASP using make (only single thread build with is supported).</p>"}]}